{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afcaed0-3d55-4e1f-95d3-c32c751c29d8",
   "metadata": {},
   "source": "# 적응형 RAG (Adaptive RAG)\n\n적응형 RAG는 (1) [질의 분석](https://blog.langchain.dev/query-construction/)과 (2) [능동적/자기 수정 RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/)를 결합한 RAG 전략입니다.\n\n[논문](https://arxiv.org/abs/2403.14403)에서는 다음 3가지 방식 간 라우팅을 위한 질의 분석을 제시합니다:\n\n* 검색 없음 (No Retrieval)\n* 단일 단계 RAG (Single-shot RAG)  \n* 반복적 RAG (Iterative RAG)\n\n이를 LangGraph를 사용하여 구현해보겠습니다.\n\n우리의 구현에서는 다음 두 방식 간 라우팅을 수행합니다:\n\n* 웹 검색: 최신 사건과 관련된 질의용\n* 자기 수정 RAG: 우리의 인덱스와 관련된 질의용\n\n![Screenshot 2024-03-26 at 1.36.03 PM.png](attachment:36fa621a-9d3d-4860-a17c-5d20e6987481.png)\n\n## 📖 구현 개요\n\n본 노트북은 Adaptive-RAG 논문의 핵심 아이디어를 LangGraph로 구현하여 다음을 보여줍니다:\n- 질의 복잡도에 따른 적응형 처리\n- 품질 보장을 위한 다단계 검증 시스템\n- 자동 질의 개선 및 재시도 메커니즘"
  },
  {
   "cell_type": "markdown",
   "id": "a85501ca-eb89-4795-aeab-cdab050ead6b",
   "metadata": {},
   "source": [
    "## 설정 (Setup)\n",
    "\n",
    "먼저 필요한 패키지들을 설치하고 API 키를 설정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1a740-9fea-4a6e-8f95-fb9dbf1c80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 설치 (출력 숨김)\n",
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f204d-956f-4128-b597-2c698120edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키 설정\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"환경변수 설정 함수 - 이미 설정되지 않은 경우에만 입력 요청\"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 필수 API 키들 설정\n",
    "_set_env(\"OPENAI_API_KEY\")     # OpenAI GPT 모델 사용을 위한 키\n",
    "# _set_env(\"COHERE_API_KEY\")   # Cohere 임베딩 사용시 필요 (선택사항)\n",
    "_set_env(\"TAVILY_API_KEY\")     # 웹 검색 기능을 위한 Tavily API 키"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e04b18",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">LangGraph 개발을 위한 <a href=\"https://smith.langchain.com\">LangSmith</a> 설정</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        LangSmith에 가입하여 LangGraph 프로젝트의 문제를 신속하게 발견하고 성능을 개선하세요. LangSmith를 사용하면 트레이스 데이터로 LangGraph로 구축한 LLM 앱을 디버그, 테스트, 모니터링할 수 있습니다 — 시작 방법에 대한 자세한 내용은 <a href=\"https://docs.smith.langchain.com\">여기</a>를 참조하세요. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1c2cd-81fb-40eb-8ba1-e9197800cba6",
   "metadata": {},
   "source": [
    "## 인덱스 생성 (Create Index)\n",
    "\n",
    "OpenAI Embeddings와 Chroma 벡터 데이터베이스를 사용하여 벡터 데이터베이스를 설정합니다.  \n",
    "에이전트, 프롬프트 엔지니어링, 대형 언어 모델(LLM)과 관련된 블로그 게시물의 URL을 입력합니다.  \n",
    "검색 증강 생성(RAG)에 사용할 벡터 인덱스를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224e5ba-50ca-495a-a7fa-0f75a080e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 인덱스 구축\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "### from langchain_cohere import CohereEmbeddings  # Cohere 임베딩 사용시\n",
    "\n",
    "# 임베딩 모델 설정 (OpenAI 임베딩 사용)\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "# 인덱싱할 문서들의 URL 목록\n",
    "# Lilian Weng의 유명한 AI/ML 블로그 포스트들\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",          # AI 에이전트에 관한 포스트\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\", # 프롬프트 엔지니어링\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",     # LLM 공격 기법\n",
    "]\n",
    "\n",
    "# 웹 페이지 로딩\n",
    "print(\"웹 페이지 로딩 중...\")\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]  # 리스트 평탄화\n",
    "print(f\"총 {len(docs_list)}개 문서 로딩 완료\")\n",
    "\n",
    "# 텍스트 분할 (tiktoken 기반 - OpenAI 토크나이저 사용)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500,    # 각 청크의 최대 토큰 수\n",
    "    chunk_overlap=0    # 청크 간 겹치는 토큰 수 (0으로 설정)\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "print(f\"총 {len(doc_splits)}개 청크로 분할 완료\")\n",
    "\n",
    "# Chroma 벡터스토어에 문서 추가 및 검색기 생성\n",
    "print(\"벡터 인덱스 생성 중...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,           # 분할된 문서들\n",
    "    collection_name=\"rag-chroma\",  # 컬렉션 이름\n",
    "    embedding=embd,                 # 임베딩 모델\n",
    ")\n",
    "retriever = vectorstore.as_retriever()  # 검색기 객체 생성\n",
    "print(\"인덱스 생성 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f52b427-750c-40f8-8893-e9caab3afd8d",
   "metadata": {},
   "source": [
    "## LLM 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28baefd-a961-49b0-8394-c5478dadda1c",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">LangChain에서 Pydantic 사용</p>\n",
    "    <p>\n",
    "        이 노트북은 Pydantic v2 <code>BaseModel</code>을 사용하므로 <code>langchain-core >= 0.3</code>이 필요합니다. <code>langchain-core < 0.3</code> 사용 시 Pydantic v1과 v2 <code>BaseModel</code>의 혼재로 인한 오류가 발생합니다.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd5ac0-fa18-4ee9-8051-062a0c56268f",
   "metadata": {},
   "source": [
    "### 질의 분석을 위한 라우터 (Router for Query Analysis)\n",
    "\n",
    "라우팅부터 시작해보겠습니다. 먼저 LLM에 질의 분석을 할당합니다.\n",
    "\n",
    "RouteQuery 데이터 모델을 생성하고 LLM에 구조화된 형식으로 지정합니다. 라우팅 결정은 프롬프트에 포함되어야 합니다. 문서의 어떤 부분을 주제에 따라 RAG로 연결할지 명확히 정의해야 합니다.\n",
    "\n",
    "LLM이 RAG 문서를 다시 요약하도록 자동화할 수 있지만, 대용량 문서를 다룰 때는 자동화가 비용이 많이 들 수 있으므로 수동으로 관리하는 것이 더 비용 효율적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec9d98-f3dc-4b7f-abc0-9d01c754f2be",
   "metadata": {},
   "outputs": [],
   "source": "# 테스트 실행\nprint(\"=== 라우터 테스트 ===\")\nprint(\"질문 1: 'Who will the Bears draft first in the NFL draft?'\")\nresult1 = question_router.invoke({\"question\": \"Who will the Bears draft first in the NFL draft?\"})\nprint(f\"라우팅 결과: {result1.datasource}\")\nprint(\"\\n질문 2: 'What are the types of agent memory?'\")\nresult2 = question_router.invoke({\"question\": \"What are the types of agent memory?\"})\nprint(f\"라우팅 결과: {result2.datasource}\")\n\ndatasource='web_search'\ndatasource='vectorstore'"
  },
  {
   "cell_type": "markdown",
   "id": "cb248c94-0b0c-4d86-8565-32aa8d7424e4",
   "metadata": {},
   "source": [
    "### 검색 평가기 (Retrieval Grader)\n",
    "\n",
    "검색을 수행한 후 결과를 평가합니다. 질의에 따라 RAG를 사용하기로 처음에 결정했지만, 검색된 문서들이 만족스럽지 않을 수 있습니다. 검색된 문서들이 질의에 충분히 관련이 있는지 평가합니다.\n",
    "\n",
    "이를 위해 LLM에 의존하여 관련성을 평가하고, 이진 'yes' 또는 'no' 결정을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856801cb-f42a-44e7-956f-47845e3664ca",
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\n평가 결과: {grade_result.binary_score}\")\n\nbinary_score='yes'"
  },
  {
   "cell_type": "markdown",
   "id": "2272333e-50b2-42ab-b472-e1055a3b94a8",
   "metadata": {},
   "source": "print(f\"\\n생성된 답변:\\n{generation}\")\n\nAgent memory in LLM-powered autonomous systems consists of short-term and long-term memory. Short-term memory utilizes in-context learning for immediate tasks, while long-term memory allows agents to retain and recall information over extended periods, often using external storage for efficient retrieval. This memory structure supports the agent's ability to reflect on past actions and improve future performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272333e-50b2-42ab-b472-e1055a3b94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 답변 생성 체인 구현\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LangChain Hub에서 검증된 RAG 프롬프트 가져오기\n",
    "# 이 프롬프트는 컨텍스트와 질문을 받아 답변을 생성하도록 설계됨\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM 설정 (답변 생성용)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 문서 목록을 하나의 텍스트로 결합하는 함수\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 컨텍스트 문자열로 결합\n",
    "    \n",
    "    Args:\n",
    "        docs: 검색된 Document 객체들의 리스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 각 문서 내용을 두 줄 간격으로 결합한 문자열\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG 체인 구성: 프롬프트 → LLM → 문자열 파서\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 테스트 실행\n",
    "print(\"=== RAG 체인 테스트 ===\")\n",
    "docs_txt = format_docs(docs)  # 이전에 검색한 문서들 사용\n",
    "print(f\"컨텍스트 길이: {len(docs_txt)} 문자\")\n",
    "print(f\"질문: {question}\")\n",
    "\n",
    "# 답변 생성\n",
    "generation = rag_chain.invoke({\"context\": docs_txt, \"question\": question})\n",
    "print(f\"\\n생성된 답변:\\n{generation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ab54a-4a4f-45fa-b1c5-cea1bf4c59d5",
   "metadata": {},
   "source": [
    "### 환각 평가기 (Hallucination Grader)\n",
    "\n",
    "LLM이 검색된 사실과 비교하여 환각을 생성했는지 확인합니다.  \n",
    "LLM의 평가를 이진 'yes' 또는 'no' 형식으로 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c08d14-77a0-4eed-b882-2d636abb22a3",
   "metadata": {},
   "outputs": [],
   "source": "    print(\"❌ 답변에 환각이나 근거 없는 내용이 포함되어 있습니다.\")\n\nGradeHallucinations(binary_score='yes')"
  },
  {
   "cell_type": "markdown",
   "id": "4f58502a-c25f-4d80-a402-5583b0cd3e41",
   "metadata": {},
   "source": [
    "### 답변 평가기 (Answer Grader)\n",
    "\n",
    "마지막으로 생성된 답변이 원래 질문에 적절히 답변하는지 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded99680-437a-4c9d-b860-619c88949d84",
   "metadata": {},
   "outputs": [],
   "source": "    print(\"❌ 답변이 질문을 충분히 해결하지 못합니다.\")\n\nGradeAnswer(binary_score='yes')"
  },
  {
   "cell_type": "markdown",
   "id": "af77946c-2646-4039-86b0-e2fde1ab7459",
   "metadata": {},
   "source": [
    "### 질의 재작성 (Question Rewriting)\n",
    "\n",
    "사용자의 원래 질문이 RAG에서 직접 사용되었습니다.  \n",
    "하지만 사용자의 질문이 RAG에 적합한 형태가 아닐 수 있습니다.  \n",
    "검색을 개선하기 위해 벡터 유사성 검색에 더 적합하도록 질문을 다시 표현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75f1d7-a47a-4577-bb0d-84b504b0867e",
   "metadata": {},
   "outputs": [],
   "source": "print(f\"개선된 질문: {rewritten_question}\")\n\n'What are the key concepts and techniques related to agent memory in artificial intelligence?'"
  },
  {
   "cell_type": "markdown",
   "id": "d07c0b31-b919-4498-869f-9673125c2473",
   "metadata": {},
   "source": [
    "## 웹 검색 도구 (Web Search Tool)\n",
    "\n",
    "웹에서 정보를 얻기 위해 Tavily Search 도구를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d829bb-1074-4976-b650-ead41dcb9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 웹 검색 도구 설정\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Tavily 웹 검색 도구 초기화\n",
    "# k=3: 상위 3개 검색 결과만 반환\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "print(\"✅ 웹 검색 도구 설정 완료\")\n",
    "print(\"- 검색 엔진: Tavily\")\n",
    "print(\"- 최대 결과 수: 3개\")\n",
    "print(\"- 사용 목적: 최신 정보 및 실시간 데이터 검색\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbbff0e-8843-45bb-b2ff-137bef707ef4",
   "metadata": {},
   "source": [
    "## 그래프 구성 (Construct the Graph)\n",
    "\n",
    "흐름을 그래프로 캡처합니다.\n",
    "\n",
    "### 그래프 상태 정의 (Define Graph State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e723fcdb-06e6-402d-912e-899795b78408",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 그래프 상태 정의\n",
    "\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    우리 그래프의 상태를 나타냅니다.\n",
    "\n",
    "    Attributes:\n",
    "        question: 사용자 질문\n",
    "        generation: LLM 생성 답변\n",
    "        documents: 검색된 문서 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    question: str          # 현재 처리 중인 질문\n",
    "    generation: str        # 생성된 답변\n",
    "    documents: List[str]   # 검색된 관련 문서들\n",
    "\n",
    "print(\"✅ 그래프 상태 클래스 정의 완료\")\n",
    "print(\"상태 구성 요소:\")\n",
    "print(\"- question: 사용자의 질문\")\n",
    "print(\"- generation: AI가 생성한 답변\")\n",
    "print(\"- documents: 검색된 참조 문서들\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d6c0d-42e8-4399-9751-e315be16607a",
   "metadata": {},
   "source": [
    "### 그래프 플로우 정의 (Define Graph Flow)\n",
    "\n",
    "각 노드와 엣지의 동작을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b5ec3-0720-443d-85b1-c0e79659ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 그래프 노드 함수들 정의\n",
    "\n",
    "from pprint import pprint\n",
    "from langchain.schema import Document\n",
    "\n",
    "# === 핵심 처리 노드들 ===\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    벡터스토어에서 문서 검색\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        state (dict): documents 키에 검색된 문서들이 추가된 새로운 상태\n",
    "    \"\"\"\n",
    "    print(\"---문서 검색 수행---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 벡터스토어에서 관련 문서 검색\n",
    "    documents = retriever.invoke(question)\n",
    "    print(f\"검색된 문서 수: {len(documents)}개\")\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    검색된 문서들을 바탕으로 답변 생성\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        state (dict): generation 키에 LLM 생성 답변이 추가된 새로운 상태\n",
    "    \"\"\"\n",
    "    print(\"---답변 생성 수행---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG 방식으로 답변 생성\n",
    "    docs_txt = format_docs(documents)\n",
    "    generation = rag_chain.invoke({\"context\": docs_txt, \"question\": question})\n",
    "    print(f\"생성된 답변 길이: {len(generation)}자\")\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    검색된 문서들이 질문과 관련성이 있는지 판단합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        state (dict): 관련성이 있는 문서들만 필터링하여 documents 키 업데이트\n",
    "    \"\"\"\n",
    "    print(\"---문서 관련성 검사---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 각 문서별로 관련성 점수 계산\n",
    "    filtered_docs = []\n",
    "    for i, d in enumerate(documents):\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(f\"---평가: 문서 {i+1} 관련성 있음---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(f\"---평가: 문서 {i+1} 관련성 없음---\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"필터링 결과: {len(documents)}개 → {len(filtered_docs)}개\")\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    더 나은 검색을 위해 질문을 변환합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        state (dict): 재작성된 질문으로 question 키 업데이트\n",
    "    \"\"\"\n",
    "    print(\"---질의 변환 수행---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 질문 재작성\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(f\"원래 질문: {question}\")\n",
    "    print(f\"개선된 질문: {better_question}\")\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    재작성된 질문을 바탕으로 웹 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        state (dict): 웹 검색 결과로 documents 키 업데이트\n",
    "    \"\"\"\n",
    "    print(\"---웹 검색 수행---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 웹 검색 실행\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    print(f\"웹 검색 결과 길이: {len(web_results.page_content)}자\")\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}\n",
    "\n",
    "\n",
    "# === 조건부 분기 함수들 ===\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    질문을 웹 검색 또는 RAG로 라우팅합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        str: 호출할 다음 노드 이름\n",
    "    \"\"\"\n",
    "    print(\"---질의 라우팅---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    \n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---질의를 웹 검색으로 라우팅---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---질의를 RAG로 라우팅---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    답변 생성을 할지, 아니면 질문을 재생성할지 결정합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        str: 호출할 다음 노드에 대한 이진 결정\n",
    "    \"\"\"\n",
    "    print(\"---평가된 문서들 검토---\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # 모든 문서가 관련성 검사에서 필터링됨\n",
    "        # 새로운 질의를 재생성할 것\n",
    "        print(\"---결정: 모든 문서가 질문과 관련 없음, 질의 변환 수행---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # 관련 문서가 있으므로 답변 생성\n",
    "        print(\"---결정: 관련 문서 존재, 답변 생성---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    생성된 답변이 문서에 근거하고 있고 질문에 답변하는지 결정합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 그래프 상태\n",
    "\n",
    "    Returns:\n",
    "        str: 호출할 다음 노드에 대한 결정\n",
    "    \"\"\"\n",
    "    print(\"---환각 검사---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # 환각 검사\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---결정: 생성 결과가 문서에 근거함---\")\n",
    "        # 질문-답변 적합성 검사\n",
    "        print(\"---질문 대비 생성 결과 평가---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---결정: 생성 결과가 질문에 적절히 답변함---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---결정: 생성 결과가 질문에 답변하지 못함---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---결정: 생성 결과가 문서에 근거하지 않음, 재시도---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "print(\"✅ 모든 그래프 노드 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab01f36-5628-49ab-bfd3-84bb6f1a1b0f",
   "metadata": {},
   "source": [
    "### 그래프 컴파일 (Compile Graph)\n",
    "\n",
    "정의된 노드들과 엣지들을 하나의 실행 가능한 워크플로로 조합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67854e07-9293-4c3c-bf9a-bc9a605570ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 그래프 워크플로 구성 및 컴파일\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# StateGraph 인스턴스 생성\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# === 노드 추가 ===\n",
    "# 각 노드는 특정 기능을 수행하는 함수와 연결됩니다\n",
    "workflow.add_node(\"web_search\", web_search)           # 웹 검색 노드\n",
    "workflow.add_node(\"retrieve\", retrieve)               # 벡터스토어 검색 노드\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # 문서 관련성 평가 노드\n",
    "workflow.add_node(\"generate\", generate)               # 답변 생성 노드\n",
    "workflow.add_node(\"transform_query\", transform_query) # 질의 변환 노드\n",
    "\n",
    "# === 엣지 구성 ===\n",
    "# 조건부 엣지: 시작점에서 라우팅 결정\n",
    "workflow.add_conditional_edges(\n",
    "    START,           # 시작점\n",
    "    route_question,  # 라우팅 함수\n",
    "    {\n",
    "        \"web_search\": \"web_search\",    # 웹 검색 경로\n",
    "        \"vectorstore\": \"retrieve\",     # 벡터스토어 검색 경로\n",
    "    },\n",
    ")\n",
    "\n",
    "# 고정 엣지: 웹 검색 → 답변 생성\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "\n",
    "# 고정 엣지: 문서 검색 → 문서 평가\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# 조건부 엣지: 문서 평가 결과에 따른 분기\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",    # 문서 평가 노드\n",
    "    decide_to_generate,   # 생성 여부 결정 함수\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",  # 질의 변환 경로\n",
    "        \"generate\": \"generate\",                # 답변 생성 경로\n",
    "    },\n",
    ")\n",
    "\n",
    "# 고정 엣지: 질의 변환 → 재검색\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "\n",
    "# 조건부 엣지: 답변 생성 후 품질 평가\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",                                    # 답변 생성 노드\n",
    "    grade_generation_v_documents_and_question,    # 품질 평가 함수\n",
    "    {\n",
    "        \"not supported\": \"generate\",        # 재생성 (환각 발견시)\n",
    "        \"useful\": END,                      # 종료 (성공)\n",
    "        \"not useful\": \"transform_query\",   # 질의 변환 후 재시도\n",
    "    },\n",
    ")\n",
    "\n",
    "# 워크플로 컴파일 - 실행 가능한 앱으로 변환\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"✅ 적응형 RAG 워크플로 컴파일 완료\")\n",
    "print(\"\\n🔄 워크플로 구조:\")\n",
    "print(\"1. START → 질의 라우팅 (웹검색 vs 벡터스토어)\")\n",
    "print(\"2. 벡터스토어 경로: 검색 → 문서평가 → 생성 여부 결정\")\n",
    "print(\"3. 웹검색 경로: 웹검색 → 답변생성\")\n",
    "print(\"4. 품질 검증: 환각검사 → 답변적합성검사 → END\")\n",
    "print(\"5. 자동 재시도: 품질 미달시 질의변환 → 재검색\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bce541",
   "metadata": {},
   "source": [
    "## 그래프 사용 (Use Graph)\n",
    "\n",
    "구성된 적응형 RAG 시스템을 실제로 테스트해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acc541-d726-4b75-84d1-a215845fe88a",
   "metadata": {},
   "outputs": [],
   "source": "# 최종 생성 결과 출력\nprint(\"📝 최종 답변:\")\npprint(value[\"generation\"])\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n---ROUTE QUESTION---\n---ROUTE QUESTION TO WEB SEARCH---\n---WEB SEARCH---\n\"Node 'web_search':\"\n'\\n---\\n'\n---GENERATE---\n---CHECK HALLUCINATIONS---\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n---GRADE GENERATION vs QUESTION---\n---DECISION: GENERATION ADDRESSES QUESTION---\n\"Node 'generate':\"\n'\\n---\\n'\n('The Chicago Bears are expected to draft quarterback Caleb Williams first '\n 'overall in the 2024 NFL Draft. They also have a second first-round pick, '\n 'where they selected wide receiver Rome Odunze.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a985dd-03c6-45af-a67b-b15746a2cb5f",
   "metadata": {},
   "outputs": [],
   "source": "print(\"- 자동 라우팅 및 자기 수정 메커니즘 확인됨\")\n\n---ROUTE QUESTION---\n---ROUTE QUESTION TO RAG---\n---RETRIEVE---\n\"Node 'retrieve':\"\n'\\n---\\n'\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\n---GRADE: DOCUMENT NOT RELEVANT---\n---GRADE: DOCUMENT RELEVANT---\n---GRADE: DOCUMENT NOT RELEVANT---\n---GRADE: DOCUMENT RELEVANT---\n---ASSESS GRADED DOCUMENTS---\n---DECISION: GENERATE---\n\"Node 'grade_documents':\"\n'\\n---\\n'\n---GENERATE---\n---CHECK HALLUCINATIONS---\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n---GRADE GENERATION vs QUESTION---\n---DECISION: GENERATION ADDRESSES QUESTION---\n\"Node 'generate':\"\n'\\n---\\n'\n('The types of agent memory include short-term memory, long-term memory, and '\n 'sensory memory. Short-term memory is utilized for in-context learning, while '\n 'long-term memory allows for the retention and recall of information over '\n 'extended periods. Sensory memory involves learning embedding representations '\n 'for various raw inputs, such as text and images.')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}